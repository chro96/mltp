<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>Logistic Regression - Machine Learning engineer Training Program</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Logistic Regression";
    var mkdocs_page_input_path = "section1/unit6/logistic_regression.md";
    var mkdocs_page_url = "/section1/unit6/logistic_regression/";
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js"></script>
  <script src="../../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> Machine Learning engineer Training Program</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../../..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Traditional Machine Learning</span>
    <ul class="subnav">
                <li class="">
                    
    <span class="caption-text">Unit1</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../unit1/introduction/">Introduction</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="../../unit1/k-nn/">k-Nearest Neighbors</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Unit2</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../unit2/supervised_learning/">Supervised Learning</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Unit3</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../unit3/naive_bayes/">Naive Bayes</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Unit4</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../unit4/k-means/">K-Means Clustering</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Unit5</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="../../unit5/linear_regression/">Linear Regression</a>
                </li>
    </ul>
                </li>
                <li class=" current">
                    
    <span class="caption-text">Unit6</span>
    <ul class="subnav">
                <li class="toctree-l3 current">
                    
    <a class="current" href="./">Logistic Regression</a>
    <ul class="subnav">
            
    <li class="toctree-l4"><a href="#logistic-regression">Logistic Regressionの説明</a></li>
    

    <li class="toctree-l4"><a href="#exercises">Exercises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#exercise-1">Exercise 1</a></li>
        
            <li><a class="toctree-l5" href="#exercise-2">Exercise 2</a></li>
        
            <li><a class="toctree-l5" href="#exercise3">Exercise3</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#one-vs-restmulticlass-classification">One vs Restを使ったMulticlass Classification</a></li>
    

    <li class="toctree-l4"><a href="#exercises_1">Exercises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#exercise-4">Exercise 4</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#softmaxmulticlass-classification">Softmaxを使ったMulticlass Classification</a></li>
    

    <li class="toctree-l4"><a href="#exercises_2">Exercises</a></li>
    
        <ul>
        
            <li><a class="toctree-l5" href="#exercise-5">Exercise 5</a></li>
        
            <li><a class="toctree-l5" href="#exercise-6">Exercise 6</a></li>
        
            <li><a class="toctree-l5" href="#exercise-7">Exercise 7</a></li>
        
            <li><a class="toctree-l5" href="#exercise-8">Exercise 8</a></li>
        
        </ul>
    

    <li class="toctree-l4"><a href="#reference">Reference</a></li>
    

    </ul>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">Machine Learning engineer Training Program</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Docs</a> &raquo;</li>
    
      
        
          <li>Traditional Machine Learning &raquo;</li>
        
      
        
          <li>Unit6 &raquo;</li>
        
      
    
    <li>Logistic Regression</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/itandi/mltp/edit/master/docs/section1/unit6/logistic_regression.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>Logistic RegressionはClassificationのモデルです。Linear Regressionと非常に似ています。</p>
<h1 id="logistic-regression">Logistic Regressionの説明</h1>
<p>まずはBinary Classification（２クラス）専用のLogistic Regressionの説明をします。
<code>__init__</code>と<code>fit</code>の始めはLinear Regressionと全く同じコードです。</p>
<pre><code class="python"># s1_initial_code.py

class MyLogisticRegression(object):
    def __init__(self, eta=0.1, n_iter=50):
        self.eta = eta
        self.n_iter = n_iter

    def fit(self, X, y):
        X = np.insert(X, 0, 1, axis=1)
        self.w = np.ones(X.shape[1])
        m = X.shape[0]
        return self
</code></pre>

<p>Linear Regressionと同じ<code>predict</code>を使うとどうなるでしょうか。</p>
<pre><code class="python">    def predict(self, X):
        return np.insert(X, 0, 1, axis=1).dot(self.w)

X = np.array([[-2, 2],[-3, 0],[2, -1],[1, -4]])
y = np.array([1,1,0,0])
logi = MyLogisticRegression().fit(X, y)
print(logi.predict(X))
</code></pre>

<pre><code class="output">[ 1. -2.  2. -2.]
</code></pre>

<p>アウトプットは０か１でなければいけないので、このままではダメですよね。これを解決する為に<em>sigmoid function</em>を使います。</p>
<p>
<script type="math/tex; mode=display"> g(x) = \frac{1}{1 + e^{-x}} </script>
</p>
<p><img alt="Sigmoid_Function.png (28.8 kB)" src="https://img.esa.io/uploads/production/attachments/5475/2017/04/10/18257/99561c38-f845-4709-a9fb-76513bfc85d4.png" /></p>
<p>sigmoid functionはxが<script type="math/tex">\infty</script>に近づくにつれ1に近づき、<script type="math/tex">-\infty</script>に近づくにつれ0に近づきます。つまりどんな値も0~1の間に収まります。これを確率と捉えることも出来ます。よって<script type="math/tex">g(x) >= 0.5</script>の場合は1、<script type="math/tex">g(x) < 0.5</script>の場合は0とします。</p>
<p>アップデートの数式は以下の通りです。Linear Regressionと全く同じに見えます。</p>
<p>
<script type="math/tex; mode=display"> \theta := \theta + \frac{\alpha}{m} (y - h(x))X </script>
</p>
<p>しかしLinear Regressionでは<script type="math/tex">h(x) = X\theta</script>なのに対し、Logistic Regressionでは<script type="math/tex">h(x) = g(X\theta)</script>です。Linear Regressionでは<script type="math/tex">h(x) = X\theta</script>がどれだけターゲットの値とずれているかがエラーでした。Logistic Regressionでは<script type="math/tex">h(x) = g(X\theta)</script>、つまりSigmoidを通した値とターゲットがどれだけずれているかをエラーとします。</p>
<p>例えば、<script type="math/tex">h(x)</script>が0.7でターゲットが1だとすると、エラーが<script type="math/tex">|1 - 0.7| = 0.3</script>あるので、これを頑張って0にしようとします。<code>predict</code>では0.5以上あれば1とするので0.7をこれ以上1に近づけなくても精度には変わらないんですが、最適化の際は精度は全く気にせずエラーを少なくしようとします。</p>
<h1 id="exercises">Exercises</h1>
<h2 id="exercise-1">Exercise 1</h2>
<p>Sigmoid functionを書きましょう。</p>
<p>
<script type="math/tex; mode=display"> g(x) = \frac{1}{1 + e^{-x}} </script>
</p>
<p>インプットがarrayなので、ループを使わずnumpyで一気に計算しましょう。</p>
<h2 id="exercise-2">Exercise 2</h2>
<p><code>predict</code>を完成させましょう。<script type="math/tex">g(x) >= 0.5</script>の場合は1、<script type="math/tex">g(x) < 0.5</script>の場合は0です。</p>
<h2 id="exercise3">Exercise3</h2>
<p>最後に<code>fit</code>を完成させましょう。Linear Regressionを参照すれば問題ないと思います。<code>_sigmoid</code>を忘れずに使いましょう。</p>
<h1 id="one-vs-restmulticlass-classification">One vs Restを使ったMulticlass Classification</h1>
<p>上記はBinary Classificationにしか使うことが出来ません。Logistic Regressionで複数のクラスを扱う方法は２つあります。一つは"One vs Rest"という手法です。これはLogistic Regressionに限らずどんなモデルにも使うことが出来ます。</p>
<p>方法はシンプルで、クラスが３つあったら３つのBinary Classifierを作ります。１つ目では0のクラスを1とし、それ以外のクラスを0にします。２つ目、３つ目でも同じ流れで擬似的にクラスを0と1の２つだけにします。</p>
<table>
<thead>
<tr>
<th>Classifier \ クラス</th>
<th>0</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<td>１つ目</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>２つ目</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>３つ目</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<pre><code class="python"># s5_exercise4.py

class LogisticRegressionOVR(object):
    &quot;&quot;&quot;One vs Rest&quot;&quot;&quot;

    def __init__(self, num_classes, eta=0.1, n_iter=50):
        self.num_classes = num_classes
        self.eta = eta
        self.n_iter = n_iter

    def fit(self, X, y):
        X = np.insert(X, 0, 1, axis=1)
        self.w = np.zeros((X.shape[1], self.num_classes))
        m = X.shape[0]

        for i in range(self.num_classes):
            # yを1と0だけにする
            y_copy = np.where(y == i, 1, 0)
            w = np.ones(X.shape[1])

            for j in range(self.n_iter):
                output = X.dot(w)
                errors = y_copy - self._sigmoid(output)
                w += self.eta / m * errors.dot(X)

                if j % 10 == 0:
                    print(sum(errors**2))
            self.w[:, i] = w

        return self
</code></pre>

<p>そして<code>predict</code>の時に３つのBinary Classifierを実行しアウトプットが一番高いものを採用します。</p>
<h1 id="exercises_1">Exercises</h1>
<h2 id="exercise-4">Exercise 4</h2>
<p><code>predict</code>を書きましょう。matrix multiplicationと<code>np.argmax</code>を使えばループを全く使わずに書けます。</p>
<h1 id="softmaxmulticlass-classification">Softmaxを使ったMulticlass Classification</h1>
<p>One vs Restはクラスの数だけBinary Classifierを学習しなければならないため時間がかかります。よって実際Logistic RegressionやNeural NetworkでMulticlass Classificationをする場合はほぼ確実に<em>Softmax</em>というものを使います。</p>
<p>Softmaxを説明する前にOne vs Restと共通するコードを見てみましょう。勿論<code>fit</code>はまだ途中です。</p>
<pre><code class="python">#s6_softmax_initial_code.py

class LogisticRegressionSoftmax(object):
    def __init__(self, num_classes, eta=0.1, n_iter=50):
        self.num_classes = num_classes
        self.eta = eta
        self.n_iter = n_iter

    def fit(self, X, y):
        X = np.insert(X, 0, 1, axis=1)
        self.w = np.random.randn(X.shape[1], self.num_classes)
        m = X.shape[0]
        return self

    def predict(self, X):
        X = np.insert(X, 0, 1, axis=1)
        return np.argmax(X.dot(self.w), axis=1)

iris = datasets.load_iris()

x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=.4)
logi = LogisticRegressionSoftmax(len(np.unique(iris.target)), n_iter=500)
logi.fit(x_train, y_train)
print(logi.w.shape)
print(logi.predict(x_test[:2]))
</code></pre>

<p><code>self.w</code>のサイズはOne vs Restと同じく(5, 3)です。weightの初期値に<code>np.random.randn</code>を使ってますが、<code>np.zeros</code>でも<code>np.ones</code>でも構いません。初期値によって最適化の速度が変わったりしますが、今はそこまで気にしなくて大丈夫です。</p>
<pre><code class="output">(5, 3)
[0 0]
</code></pre>

<p>以下が<code>fit</code>の完成形です。<code>X.dot(self.w)</code>のサイズは(m, クラス数)です。これにsoftmaxを当て、各クラスの確率を出します。</p>
<pre><code class="python"># s7_exercise5.py

def fit(self, X, y):
    X = np.insert(X, 0, 1, axis=1)
    y = self._one_hot(y)
    self.w = np.random.randn(X.shape[1], self.num_classes)
    m = X.shape[0]

    for i in range(self.n_iter):
        output = self._softmax(X.dot(self.w))
        errors = y - output
        self.w += self.eta / m * X.T.dot(errors)

        if i % 10 == 0:
            print(self._cross_entropy(y, output))
    return self
</code></pre>

<p>以下がSoftmaxの公式です。</p>
<p>
<script type="math/tex; mode=display"> \sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} </script>
</p>
<p>これをSigmoidと同じように線形関数<code>X.dot(self.w)</code>の後に使います。Sigmoidのインプットが数字なのに対し、Softmaxのインプットはarrayになります（コードでは全サンプル一度に計算するのでSigmoidのインプットもarrayですが、ここでは１サンプルでの話です）。</p>
<p>式だけ見てもピンと来ないと思うので実際に計算してみましょう。
例えば<code>X.dot(self.w)</code>の１サンプルが<script type="math/tex">[2,1,-3]</script>だとします。</p>
<p>まずそれぞれの分子を計算しましょう。</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
e^2 &= 7.389 \\
e^1 &= 2.718 \\
e^{-3}&= 0.0498
\end{align}
</script>
</p>
<p>分母はこれらの合計です。</p>
<p>
<script type="math/tex; mode=display"> 7.389 + 2.718 + 0.0498 = 10.157 </script>
</p>
<p>そしてそれぞれをこの合計で割れば完成です。</p>
<p>
<script type="math/tex; mode=display"> [7.389, 2.718, 0.0498] / 10.157 = [0.727,0.268,0.005] </script>
</p>
<p>Softmaxの値は合計すると1になることが分かります。つまりこれは各クラスの確率を表しているとも言えます。「72.7％の確率でクラス０だろう」ということですね。</p>
<p>
<script type="math/tex; mode=display"> 0.727 + 0.268 + 0.005 = 1 </script>
</p>
<p>これとターゲットの差分を縮めていくのですが、そのloss functionには<em>cross entropy</em>を使います。pがターゲット、qがsoftmax後のアウトプットとするとcross entropyは以下の通りです。</p>
<p>
<script type="math/tex; mode=display"> H(p, q) = -\sum_x p(x)\, \log q(x) </script>
</p>
<p>qがarrayなので、pもarrayにしなければなりません。その為に<em>one hot encoding</em>という手段を使います。例えばクラスが全部で３つあるとし、クラスが0の場合は<script type="math/tex; mode=display">[1,0,0]</script>、クラスが2の場合は<script type="math/tex; mode=display">[0,0,1]</script>となります。</p>
<p>では上記のアウトプットのターゲットが0だとすると、<script type="math/tex; mode=display">p = [1,0,0]</script>, <script type="math/tex; mode=display">q =[0.727,0.268,0.005] </script>となるのでcross entropyは</p>
<p>
<script type="math/tex; mode=display">
\begin{align}
 H(p,q) &= - (1 \times log(0.727) + 0  \times log(0.268) + 0 \times log(0.005)) \\
&= -(1 \times -0.3188 + 0 + 0) \\
&= 0.3188
\end{align}
</script>
</p>
<p>
<script type="math/tex">p = [0,1,0]</script>,  <script type="math/tex">p = [0,0,1]</script>の場合も計算してみて下さい。<script type="math/tex">p</script>と<script type="math/tex">q</script>が近いほどcross entropyは小さくなります。なので<script type="math/tex">p = [1,0,0]</script>の時が一番小さくなるはずです。</p>
<h1 id="exercises_2">Exercises</h1>
<p><code>fit</code>の部分は微分積分を要するところですしSigmoidの時とほとんど変わらないので今回は既に完成されています。</p>
<h2 id="exercise-5">Exercise 5</h2>
<p><code>_one_hot</code>を書きましょう。インプットがarrayなので各数字毎にone_hot_encodingを作って下さい。</p>
<h2 id="exercise-6">Exercise 6</h2>
<p>softmaxを書きましょう。まずは１サンプルづつ計算します。
xが大きいと指数が物凄くなってしまいoverflowしてしまうで、実際にsoftmaxを使う時は<code>x -= np.max(x)</code>とすることによってoverflowを防ぎます。</p>
<h2 id="exercise-7">Exercise 7</h2>
<p>次に全サンプル一気に計算するsoftmaxを書きましょう。overflowの対処も書いて下さい。</p>
<h2 id="exercise-8">Exercise 8</h2>
<p><code>_cross_entropy</code>を書きましょう。全サンプル一度に計算して下さい。</p>
<h1 id="reference">Reference</h1>
<p>Softmaxとcross entropyの説明が分かり易いです。
http://cs231n.github.io/linear-classify/#softmax</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../../unit5/linear_regression/" class="btn btn-neutral" title="Linear Regression"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/itandi/mltp" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../../unit5/linear_regression/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script src="../../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
